{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw data from GCP bucket\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=1000)\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 50)\n",
    "# pd.set_option('display.width', 100)\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# The bucket name for the location of the data is in the .env file\n",
    "BUCKET_NAME = os.environ['BUCKET_NAME']\n",
    "DATA_FILE = 'labeled_npi.csv'\n",
    "df_pkl = pd.read_pickle(\"data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocesser:\n",
    "    def __init__(self, df):\n",
    "        self.df_labels = pd.read_csv(DATA_FILE)\n",
    "        self.keywords = ['incident command system',\n",
    "                         'emergency operations',\n",
    "                         'joint information center',\n",
    "                         'social distancing',\n",
    "                         'childcare closers',\n",
    "                         'travel advisory',\n",
    "                         'travel warning',\n",
    "                         'isolation',\n",
    "                         'quarantine',\n",
    "                         'mass gathering cancellations',\n",
    "                         'school closures',\n",
    "                         'facility closures',\n",
    "                         'evacuation',\n",
    "                         'relocation',\n",
    "                         'restricting travel',\n",
    "                         'travel ban',\n",
    "                         'patient cohort',\n",
    "                         'npi']\n",
    "        self.occurances_minimum = 1\n",
    "        self.df_full = df.copy()\n",
    "        print(self.df_full.shape)\n",
    "        self.key_slice()\n",
    "        print(self.df_full.shape)\n",
    "        self.npi_slice()\n",
    "        print(self.df_full.shape)\n",
    "        self.df_full = self.df_full.merge(self.df_labels, on=\"title\", how=\"inner\")\n",
    "        self.df_full = self.df_full.loc[self.df_full.isNPI.notna()]\n",
    "            \n",
    "    def key_slice(self):\n",
    "        self.df_full = self.df_full[self.df_full['abstract'].str.contains('|'.join(self.keywords), na=False, regex=True)].reset_index(drop=True)\n",
    "        \n",
    "    def npi_slice(self):\n",
    "        def get_count(row):\n",
    "            return sum([row['abstract'].count(keyword) for keyword in self.keywords])\n",
    "        self.df_full = self.df_full[self.df_full.apply(get_count, axis=1) >= self.occurances_minimum]\n",
    "        \n",
    "    def remove_stopwords(self,columns):\n",
    "        stop = stopwords.words('english')\n",
    "        for col in columns:\n",
    "            self.df_full[col] = self.df_full[col].astype(str).apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "    def to_tfidf(self,columns):\n",
    "        for col in columns:\n",
    "            tfidfv = TfidfVectorizer()\n",
    "            self.df_full[col + '_tfidf'] = list(tfidfv.fit_transform(self.df_full[col]).toarray())\n",
    "            \n",
    "    def remove_punc(self, columns):\n",
    "        for col in columns:\n",
    "            self.df_full[col] = self.df_full[col].str.replace('[^a-zA-Z\\s]+','')\n",
    "        \n",
    "def display_wordcloud(text):\n",
    "    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color='white').generate(text)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_apply(df, columns, n_comp):\n",
    "    new_df = df.copy()\n",
    "    for col in columns:\n",
    "        pca = PCA(n_components=n_comp)\n",
    "        new_df[col+'_pca'] = list(pca.fit_transform(np.stack(df[col].to_numpy())))\n",
    "    return new_df\n",
    "\n",
    "def apply_scaler(df, columns):\n",
    "    new_df = df.copy()\n",
    "    for col in columns:\n",
    "        scaler = StandardScaler()\n",
    "        new_df[col + '_scaled'] = list(scaler.fit_transform(np.stack(df[col].to_numpy())))\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1612, 6)\n",
      "(1612, 6)\n",
      "(1612, 6)\n"
     ]
    }
   ],
   "source": [
    "data_obj = Preprocesser(df_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    420\n",
       "1.0    187\n",
       "Name: isNPI, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_obj.df_full.isNPI.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obj.remove_punc(['body_text','abstract'])\n",
    "data_obj.remove_stopwords(['body_text', 'abstract'])\n",
    "data_obj.to_tfidf(['body_text', 'abstract'])\n",
    "pca_df = pca_apply(data_obj.df_full, ['abstract_tfidf','body_text_tfidf'], 50)\n",
    "scaled_df = apply_scaler(pca_df,['abstract_tfidf_pca','body_text_tfidf_pca'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paper_id', 'title', 'author_list', 'abstract', 'body_text', 'doi',\n",
       "       'Unnamed: 0', 'Unnamed: 0.1', 'isNPI', 'body_text_tfidf',\n",
       "       'abstract_tfidf', 'abstract_tfidf_pca', 'body_text_tfidf_pca',\n",
       "       'abstract_tfidf_pca_scaled', 'body_text_tfidf_pca_scaled'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(scaled_df[\"body_text_tfidf_pca_scaled\"].to_numpy())\n",
    "y = scaled_df[\"isNPI\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain.num_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest.num_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}\n",
    "param['eval_metric'] = 'auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-auc:0.87907\ttrain-auc:0.88473\n",
      "[1]\teval-auc:0.90555\ttrain-auc:0.91041\n",
      "[2]\teval-auc:0.91651\ttrain-auc:0.94459\n",
      "[3]\teval-auc:0.92841\ttrain-auc:0.95935\n",
      "[4]\teval-auc:0.91917\ttrain-auc:0.96899\n",
      "[5]\teval-auc:0.92121\ttrain-auc:0.97431\n",
      "[6]\teval-auc:0.92998\ttrain-auc:0.98291\n",
      "[7]\teval-auc:0.92199\ttrain-auc:0.98690\n",
      "[8]\teval-auc:0.93045\ttrain-auc:0.99030\n",
      "[9]\teval-auc:0.93327\ttrain-auc:0.99076\n",
      "[10]\teval-auc:0.93515\ttrain-auc:0.99240\n",
      "[11]\teval-auc:0.93390\ttrain-auc:0.99510\n",
      "[12]\teval-auc:0.92575\ttrain-auc:0.99645\n",
      "[13]\teval-auc:0.93734\ttrain-auc:0.99738\n",
      "[14]\teval-auc:0.93484\ttrain-auc:0.99797\n",
      "[15]\teval-auc:0.93108\ttrain-auc:0.99851\n",
      "[16]\teval-auc:0.93515\ttrain-auc:0.99885\n",
      "[17]\teval-auc:0.92920\ttrain-auc:0.99937\n",
      "[18]\teval-auc:0.92199\ttrain-auc:0.99917\n",
      "[19]\teval-auc:0.92857\ttrain-auc:0.99943\n",
      "[20]\teval-auc:0.92763\ttrain-auc:0.99971\n",
      "[21]\teval-auc:0.93264\ttrain-auc:0.99973\n",
      "[22]\teval-auc:0.93108\ttrain-auc:0.99967\n",
      "[23]\teval-auc:0.93327\ttrain-auc:0.99965\n",
      "[24]\teval-auc:0.93640\ttrain-auc:0.99965\n",
      "[25]\teval-auc:0.94142\ttrain-auc:0.99985\n",
      "[26]\teval-auc:0.94392\ttrain-auc:0.99981\n",
      "[27]\teval-auc:0.94424\ttrain-auc:0.99985\n",
      "[28]\teval-auc:0.94204\ttrain-auc:0.99985\n",
      "[29]\teval-auc:0.94392\ttrain-auc:0.99987\n",
      "[30]\teval-auc:0.94549\ttrain-auc:0.99985\n",
      "[31]\teval-auc:0.94580\ttrain-auc:0.99989\n",
      "[32]\teval-auc:0.94486\ttrain-auc:0.99989\n",
      "[33]\teval-auc:0.94517\ttrain-auc:0.99991\n",
      "[34]\teval-auc:0.94486\ttrain-auc:0.99991\n",
      "[35]\teval-auc:0.94236\ttrain-auc:0.99989\n",
      "[36]\teval-auc:0.94424\ttrain-auc:0.99991\n",
      "[37]\teval-auc:0.94236\ttrain-auc:0.99987\n",
      "[38]\teval-auc:0.94079\ttrain-auc:0.99989\n",
      "[39]\teval-auc:0.94142\ttrain-auc:0.99989\n",
      "[40]\teval-auc:0.94079\ttrain-auc:0.99991\n",
      "[41]\teval-auc:0.94517\ttrain-auc:0.99991\n",
      "[42]\teval-auc:0.94455\ttrain-auc:0.99991\n",
      "[43]\teval-auc:0.94455\ttrain-auc:0.99991\n",
      "[44]\teval-auc:0.94361\ttrain-auc:0.99991\n",
      "[45]\teval-auc:0.94361\ttrain-auc:0.99991\n",
      "[46]\teval-auc:0.94110\ttrain-auc:0.99991\n",
      "[47]\teval-auc:0.93922\ttrain-auc:0.99991\n",
      "[48]\teval-auc:0.94455\ttrain-auc:0.99991\n",
      "[49]\teval-auc:0.94361\ttrain-auc:0.99991\n",
      "[50]\teval-auc:0.94330\ttrain-auc:0.99991\n",
      "[51]\teval-auc:0.94486\ttrain-auc:0.99991\n",
      "[52]\teval-auc:0.94392\ttrain-auc:0.99991\n",
      "[53]\teval-auc:0.94392\ttrain-auc:0.99991\n",
      "[54]\teval-auc:0.94361\ttrain-auc:0.99991\n",
      "[55]\teval-auc:0.94455\ttrain-auc:0.99991\n",
      "[56]\teval-auc:0.94361\ttrain-auc:0.99991\n",
      "[57]\teval-auc:0.94048\ttrain-auc:0.99991\n",
      "[58]\teval-auc:0.94204\ttrain-auc:0.99991\n",
      "[59]\teval-auc:0.94298\ttrain-auc:0.99991\n",
      "[60]\teval-auc:0.94110\ttrain-auc:0.99991\n",
      "[61]\teval-auc:0.94267\ttrain-auc:0.99991\n",
      "[62]\teval-auc:0.94549\ttrain-auc:0.99991\n",
      "[63]\teval-auc:0.94455\ttrain-auc:0.99991\n",
      "[64]\teval-auc:0.94424\ttrain-auc:0.99991\n",
      "[65]\teval-auc:0.94267\ttrain-auc:0.99991\n",
      "[66]\teval-auc:0.94549\ttrain-auc:0.99991\n",
      "[67]\teval-auc:0.94517\ttrain-auc:0.99991\n",
      "[68]\teval-auc:0.94486\ttrain-auc:0.99991\n",
      "[69]\teval-auc:0.94580\ttrain-auc:0.99991\n",
      "[70]\teval-auc:0.94737\ttrain-auc:0.99991\n",
      "[71]\teval-auc:0.94517\ttrain-auc:0.99991\n",
      "[72]\teval-auc:0.94424\ttrain-auc:0.99991\n",
      "[73]\teval-auc:0.94298\ttrain-auc:0.99991\n",
      "[74]\teval-auc:0.94298\ttrain-auc:0.99991\n",
      "[75]\teval-auc:0.94361\ttrain-auc:0.99991\n",
      "[76]\teval-auc:0.94236\ttrain-auc:0.99991\n",
      "[77]\teval-auc:0.94204\ttrain-auc:0.99991\n",
      "[78]\teval-auc:0.94236\ttrain-auc:0.99991\n",
      "[79]\teval-auc:0.94173\ttrain-auc:0.99991\n",
      "[80]\teval-auc:0.94079\ttrain-auc:0.99991\n",
      "[81]\teval-auc:0.93985\ttrain-auc:0.99991\n",
      "[82]\teval-auc:0.94079\ttrain-auc:0.99991\n",
      "[83]\teval-auc:0.94236\ttrain-auc:0.99991\n",
      "[84]\teval-auc:0.94330\ttrain-auc:0.99991\n",
      "[85]\teval-auc:0.94236\ttrain-auc:0.99991\n",
      "[86]\teval-auc:0.94204\ttrain-auc:0.99991\n",
      "[87]\teval-auc:0.94267\ttrain-auc:0.99991\n",
      "[88]\teval-auc:0.94236\ttrain-auc:0.99991\n",
      "[89]\teval-auc:0.94204\ttrain-auc:0.99991\n",
      "[90]\teval-auc:0.94298\ttrain-auc:0.99991\n",
      "[91]\teval-auc:0.94361\ttrain-auc:0.99991\n",
      "[92]\teval-auc:0.94298\ttrain-auc:0.99991\n",
      "[93]\teval-auc:0.94330\ttrain-auc:0.99991\n",
      "[94]\teval-auc:0.94267\ttrain-auc:0.99991\n",
      "[95]\teval-auc:0.94298\ttrain-auc:0.99991\n",
      "[96]\teval-auc:0.94267\ttrain-auc:0.99991\n",
      "[97]\teval-auc:0.94267\ttrain-auc:0.99991\n",
      "[98]\teval-auc:0.94079\ttrain-auc:0.99991\n",
      "[99]\teval-auc:0.94079\ttrain-auc:0.99991\n"
     ]
    }
   ],
   "source": [
    "num_round = 100\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
